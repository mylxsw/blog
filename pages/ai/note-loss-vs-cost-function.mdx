
# 跟 GPT 学 AI：Loss Function vs Cost Function

import Meta from 'components/meta';

<Meta author="mylxsw" date={new Date('2024-09-03')} tags={["machine-learning"]} />

import ChatBox from 'components/chat-box';

<ChatBox type="info">
what's the difference between loss and cost function?
</ChatBox>

The **loss function** and the **cost function** are related concepts in machine learning, but they have different meanings and uses:

## Loss Function

- The **loss function** measures the error for a single training example. It tells us how well the model is performing for one instance.
- It represents the difference between the predicted value (output by the model) and the actual value (ground truth).
- For example, in regression problems, a common loss function is the **Mean Squared Error (MSE)**: 

  $$
  \text{Loss}(y, \hat{y}) = (\hat{y} - y)^2
  $$

  where $ \hat{y} $ is the predicted value and $ y $ is the actual value.

- In classification problems, the loss function often used is the **log loss** (or cross-entropy loss).

## Cost Function

- The **cost function** is the average of the loss function over the entire training dataset. It measures how well the model is performing overall.
- The cost function is used to find the optimal model parameters by minimizing the average error over all training examples.
- Mathematically, the cost function is often defined as:

  $$
  J(\theta) = \frac{1}{m} \sum_{i=1}^m \text{Loss}(y^{(i)}, \hat{y}^{(i)})
  $$

  where $ m $ is the number of training examples, and $ \theta $ represents the model parameters.

### **Key Differences:**
- **Scope:** The loss function deals with individual samples, while the cost function is an aggregated measure over the entire training dataset.
- **Purpose:** The loss function helps understand the model's performance on a single data point, whereas the cost function is used to optimize the model parameters by minimizing the overall error.
- **Terminology:** In practice, "loss function" and "cost function" are sometimes used interchangeably, but the distinction is that the cost function typically refers to the average loss across all examples.

### **Example:**
- For linear regression:
  - **Loss function** for a single data point: $ (\hat{y} - y)^2 $
  - **Cost function** (Mean Squared Error): $ J(\theta) = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2 $

To summarize, the loss function is a measure of error for a single instance, while the cost function is the average loss across all instances and is the objective that we aim to minimize during model training.